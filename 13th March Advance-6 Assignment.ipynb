{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d461c3c3-50e4-4fe3-84dd-66c955e0eeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact\n",
    "the validity of the results.\n",
    "\n",
    "\n",
    "Analysis of Variance (ANOVA) is a statistical method used to compare the means of two or more groups to determine if there are statistically significant differences among them. To use ANOVA correctly and interpret the results accurately, certain assumptions need to be met. These assumptions relate to the underlying statistical properties of the data and the method's validity. Violations of these assumptions can lead to incorrect conclusions and invalidate the results.\n",
    "\n",
    "The assumptions for ANOVA include:\n",
    "\n",
    "Independence: The observations within each group are assumed to be independent of each other. This means that the value of one observation does not influence the value of another observation within the same group.\n",
    "\n",
    "Normality: The data within each group should follow a normal distribution. This is especially important when the group sizes are small. Deviations from normality can affect the accuracy of p-values and confidence intervals.\n",
    "\n",
    "Homogeneity of Variances (Homoscedasticity): The variances of the groups should be approximately equal. In other words, the spread of the data points around the group means should be consistent across all groups. Unequal variances can lead to incorrect significance levels and affect the F-test.\n",
    "\n",
    "Examples of violations that could impact the validity of ANOVA results:\n",
    "\n",
    "Independence Violation: In a study where participants are measured over time, such as in a repeated measures design, the assumption of independence can be violated. Measurements taken from the same participant over time are likely to be correlated, potentially leading to inaccurate results if this correlation is not properly accounted for.\n",
    "\n",
    "Normality Violation: If the data within a group significantly deviates from a normal distribution, the results of ANOVA may not be reliable. For example, if the data is heavily skewed or contains outliers, the assumption of normality could be violated. In such cases, transforming the data or using non-parametric tests might be more appropriate.\n",
    "\n",
    "Homoscedasticity Violation: When the variability of the groups' data is not consistent across groups, the assumption of homogeneity of variances is violated. This can lead to unequal contributions of different groups to the overall variance, affecting the validity of the F-test. This violation can also impact the calculation of p-values.\n",
    "\n",
    "It's important to note that ANOVA is somewhat robust to violations of assumptions, especially when sample sizes are large. However, when assumptions are severely violated, the results might become unreliable, and alternative statistical methods or data transformations may be necessary.\n",
    "\n",
    "To address potential violations, researchers often conduct preliminary analyses, such as visual inspection of data distributions, residual plots, and formal tests for normality and homoscedasticity. If assumptions are significantly violated, considering alternative analyses or transformations can help ensure the validity of the results.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q2. What are the three types of ANOVA, and in what situations would each be used?\n",
    "\n",
    "ANOVA (Analysis of Variance) is a statistical technique used to analyze the differences among group means in a data set. There are three main types of ANOVA, each designed for specific situations:\n",
    "\n",
    "One-Way ANOVA:\n",
    "One-Way ANOVA is used when you have one independent variable (factor) and one dependent variable. It's used to determine whether there are any statistically significant differences among the means of three or more independent (unrelated) groups. For example, you might use a One-Way ANOVA to compare the average scores of students from different schools to determine if there's a significant difference in performance among those schools.\n",
    "\n",
    "Two-Way ANOVA:\n",
    "Two-Way ANOVA is an extension of the One-Way ANOVA and is used when you have two independent variables. This type of ANOVA is used to examine the interaction effects between two factors and their influence on a dependent variable. One factor is typically referred to as the \"rows\" or \"between-groups\" factor, and the other as the \"columns\" or \"within-groups\" factor. Two-Way ANOVA can help analyze how two different factors impact the dependent variable, as well as any potential interaction effects between those factors. For instance, you might use Two-Way ANOVA to investigate the effects of both gender and age on the response time of participants in a cognitive task.\n",
    "\n",
    "MANOVA (Multivariate Analysis of Variance):\n",
    "MANOVA is used when there are multiple dependent variables and two or more independent variables. It allows you to analyze the relationship between multiple dependent variables and multiple independent variables simultaneously. MANOVA is useful when you want to understand how multiple factors influence multiple response variables together. For instance, in a medical study, you might use MANOVA to examine how different treatments (independent variables) affect various health-related outcomes (dependent variables) in patients.\n",
    "\n",
    "In summary, the three types of ANOVA serve different purposes based on the number of independent variables and dependent variables in your study. One-Way ANOVA is for single independent variable and single dependent variable scenarios, Two-Way ANOVA deals with two independent variables and one dependent variable, and MANOVA addresses situations with multiple dependent and independent variables.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?\n",
    "\n",
    "In Analysis of Variance (ANOVA), the partitioning of variance refers to the process of decomposing the total variability observed in a dataset into different sources of variation. ANOVA is a statistical technique used to analyze the differences among group means in a way that helps determine whether these differences are statistically significant or if they could have occurred by random chance.\n",
    "\n",
    "The partitioning of variance involves dividing the total variance into several components, each representing a different source of variation:\n",
    "\n",
    "Total Variance (TSS): This is the total variability in the data across all observations. It is calculated as the sum of squared differences between each observation and the overall mean of the entire dataset.\n",
    "\n",
    "Between-Group Variance (BSS): Also known as the \"between-treatments\" variance, this component represents the variability between different groups (or treatments) being compared. It is calculated as the sum of squared differences between the group means and the overall mean.\n",
    "\n",
    "Within-Group Variance (WSS): Also known as the \"within-treatments\" or \"error\" variance, this component represents the variability within each group. It measures the spread of individual observations around their respective group means.\n",
    "\n",
    "The key formula for partitioning variance is:\n",
    "    \n",
    "    TSS = BSS + WSS\n",
    "\n",
    "\n",
    "Understanding the partitioning of variance is important for several reasons:\n",
    "\n",
    "1.Identifying Sources of Variation: ANOVA helps to identify whether the differences observed among groups are due to genuine treatment effects (between-group variation) or just random fluctuations (within-group variation).\n",
    "\n",
    "2.Statistical Significance: By comparing the magnitudes of between-group and within-group variances, ANOVA allows us to determine whether the observed between-group differences are statistically significant. This helps researchers make informed decisions about whether to reject or fail to reject the null hypothesis.\n",
    "\n",
    "3.Effect Size Estimation: The partitioning of variance provides insights into the size of the effect that different treatments or groups have on the dependent variable. Effect size is a measure of the practical significance of the observed differences.\n",
    "\n",
    "4.Experimental Design Evaluation: Researchers can use ANOVA to assess the effectiveness of their experimental designs. If a large proportion of the total variance is explained by the between-group variation, it suggests that the experimental manipulation is having a substantial effect.\n",
    "\n",
    "5.Model Validation: ANOVA is a fundamental tool in model validation and hypothesis testing, helping researchers make informed conclusions based on the data at hand.\n",
    "\n",
    "\n",
    "Q4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual\n",
    "sum of squares (SSR) in a one-way ANOVA using Python?\n",
    "\n",
    "\n",
    "Calculating the Total Sum of Squares (SST), Explained Sum of Squares (SSE), and Residual Sum of Squares (SSR) in a one-way ANOVA using Python involves a few steps. Here's a general outline of the process:\n",
    "\n",
    "Calculate the Overall Mean (Grand Mean):\n",
    "Calculate the mean of all the data points across all groups.\n",
    "\n",
    "Calculate the Group Means:\n",
    "Calculate the mean of each individual group.\n",
    "\n",
    "Calculate the Total Sum of Squares (SST):\n",
    "SST represents the total variability in the data. It's the sum of the squared differences between each data point and the overall mean.\n",
    "\n",
    "Calculate the Explained Sum of Squares (SSE):\n",
    "SSE represents the variability explained by the differences between the group means and the overall mean.\n",
    "\n",
    "Calculate the Residual Sum of Squares (SSR):\n",
    "SSR represents the variability that is not explained by the differences between the group means. It's the sum of the squared differences between each data point and its respective group mean.\n",
    "\n",
    "Here's a Python code example using the numpy library to calculate these values:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Sample data for each group\n",
    "group1 = np.array([12, 15, 18, 20, 25])\n",
    "group2 = np.array([28, 30, 32, 35, 38])\n",
    "group3 = np.array([42, 45, 48, 50, 55])\n",
    "\n",
    "# Overall data\n",
    "data = np.concatenate([group1, group2, group3])\n",
    "\n",
    "# Calculate overall mean\n",
    "overall_mean = np.mean(data)\n",
    "\n",
    "# Calculate group means\n",
    "group_means = [np.mean(group) for group in [group1, group2, group3]]\n",
    "\n",
    "# Calculate Total Sum of Squares (SST)\n",
    "sst = np.sum((data - overall_mean)**2)\n",
    "\n",
    "# Calculate Explained Sum of Squares (SSE)\n",
    "sse = np.sum([len(group) * (mean - overall_mean)**2 for group, mean in zip([group1, group2, group3], group_means)])\n",
    "\n",
    "# Calculate Residual Sum of Squares (SSR)\n",
    "ssr = sst - sse\n",
    "\n",
    "print(\"Total Sum of Squares (SST):\", sst)\n",
    "print(\"Explained Sum of Squares (SSE):\", sse)\n",
    "print(\"Residual Sum of Squares (SSR):\", ssr)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?\n",
    "\n",
    "In a two-way ANOVA (Analysis of Variance), you're interested in analyzing the effects of two independent categorical variables (factors) on a continuous dependent variable. The main effects represent the impact of each factor individually, while the interaction effect represents how the combination of factors influences the dependent variable. You can perform a two-way ANOVA in Python using libraries like SciPy and statsmodels.\n",
    "\n",
    "Here's an outline of how to calculate main effects and interaction effects using Python:\n",
    "\n",
    "1.Data Preparation:\n",
    "Make sure you have your data organized in a suitable format. You typically need a DataFrame where each column corresponds to a factor and the dependent variable.\n",
    "\n",
    "2.Import Required Libraries:\n",
    "    \n",
    "import pandas as pd\n",
    "from scipy.stats import f_oneway\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "3.Perform One-Way ANOVAs for Main Effects:\n",
    "For each factor, perform a one-way ANOVA to calculate the main effect. Here's an example assuming you have factors A and B:\n",
    "\n",
    "# Assuming 'data' is your DataFrame and 'dependent_variable' is the column name of the dependent variable\n",
    "a_levels = data['A'].unique()\n",
    "b_levels = data['B'].unique()\n",
    "\n",
    "main_effect_a = {}\n",
    "main_effect_b = {}\n",
    "\n",
    "for level in a_levels:\n",
    "    subset = data[data['A'] == level][dependent_variable]\n",
    "    main_effect_a[level] = subset\n",
    "\n",
    "for level in b_levels:\n",
    "    subset = data[data['B'] == level][dependent_variable]\n",
    "    main_effect_b[level] = subset\n",
    "\n",
    "\n",
    "4.Perform Two-Way ANOVA and Calculate Interaction Effect:\n",
    "Use the statsmodels library to perform the two-way ANOVA and calculate the interaction effect. Here's an example assuming you have factors A and B:\n",
    "\n",
    "model = ols(f'{dependent_variable} ~ C(A) + C(B) + C(A):C(B)', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "interaction_effect = anova_table.loc['C(A):C(B)', 'F']\n",
    "\n",
    "The interaction effect is represented by the F-statistic associated with the interaction term in the ANOVA table.\n",
    "\n",
    "Remember that these code snippets are meant to provide a general idea of the process. Depending on your data structure and specific requirements, you may need to adapt and customize the code accordingly. Also, consider performing post-hoc tests to further analyze the differences between factor levels if the ANOVA results are significant.\n",
    "\n",
    "\n",
    "Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02.\n",
    "What can you conclude about the differences between the groups, and how would you interpret these\n",
    "results?\n",
    "\n",
    "A one-way ANOVA (Analysis of Variance) is a statistical test used to compare means of three or more groups to determine if there are statistically significant differences among the groups. The F-statistic is a test statistic that helps in making this determination, and the associated p-value indicates the probability of observing the obtained results under the assumption that there are no true differences among the group means.\n",
    "\n",
    "In your scenario, you obtained an F-statistic of 5.23 and a p-value of 0.02. Let's break down what this means:\n",
    "\n",
    "F-statistic: The F-statistic is calculated by comparing the variability between group means (explained variability) with the variability within the groups (unexplained variability). A larger F-statistic suggests that the group means are more different from each other compared to the variability within the groups.\n",
    "\n",
    "p-value: The p-value associated with the F-statistic indicates the probability of obtaining the observed results if the null hypothesis is true. In the context of ANOVA, the null hypothesis states that there are no significant differences among the group means.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "Given the F-statistic of 5.23 and a p-value of 0.02:\n",
    "\n",
    "P-value Interpretation: The p-value of 0.02 is below the commonly used significance threshold of 0.05. This suggests that the probability of observing the obtained data if there were no true differences among the group means is quite low (less than 2%).\n",
    "\n",
    "Conclusion: With a p-value below 0.05, you would typically reject the null hypothesis. This means that there is sufficient evidence to suggest that there are statistically significant differences among at least some of the groups' means.\n",
    "\n",
    "Effect Size: While the p-value indicates statistical significance, it's also important to consider the effect size. The effect size helps you understand the practical significance of the differences between group means. You might consider calculating measures like eta-squared or Cohen's d to quantify the effect size.\n",
    "\n",
    "In summary, based on the F-statistic and p-value you provided, you can conclude that there are statistically significant differences among the groups' means. However, remember that statistical significance doesn't necessarily imply practical or meaningful significance. Further post-hoc tests or pairwise comparisons might be conducted to determine which specific group means are significantly different from each other.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential\n",
    "consequences of using different methods to handle missing data?\n",
    "\n",
    "\n",
    "Handling missing data in a repeated measures ANOVA is crucial to ensure the validity of your analysis and the reliability of your results. There are several methods to deal with missing data, each with its own potential consequences:\n",
    "\n",
    "Listwise Deletion (Complete Case Analysis): This involves excluding any participant or case that has missing data on any of the variables involved in the analysis. While this method is straightforward, it can lead to reduced sample size, loss of statistical power, and potential bias if the missing data are not random.\n",
    "\n",
    "Pairwise Deletion: This method includes all available data for each specific pairwise comparison within the repeated measures design. While it retains more data than listwise deletion, it can still lead to biased results if the missing data are related to the variables being analyzed.\n",
    "\n",
    "Imputation Methods: Imputation involves estimating missing values based on observed data. There are different imputation methods, each with its own set of consequences:\n",
    "\n",
    "Mean/Median Imputation: Replacing missing values with the mean or median of the observed values for that variable. This can lead to an underestimation of variability and potentially distort relationships in the data.\n",
    "Regression Imputation: Predicting missing values based on the relationship with other variables using regression analysis. While this can capture more complex relationships, it assumes that the relationship between the variables is linear and can introduce bias if the assumption is not met.\n",
    "Multiple Imputation: Creating multiple plausible imputed datasets and conducting the analysis separately on each dataset before combining the results. This method accounts for the uncertainty introduced by imputation and provides more robust estimates. However, it can be computationally intensive and requires assumptions about the missing data mechanism.\n",
    "Last Observation Carried Forward (LOCF) or Next Observation Carried Backward (NOCB): These methods involve using the last observed value before a missing value or the next observed value after a missing value to fill in the gaps. While these methods are simple, they can introduce bias if the missing data pattern is related to the variable's trajectory over time.\n",
    "\n",
    "Pattern-Mixture Models and Mixed-Effects Models: These advanced statistical methods incorporate the missing data mechanism into the analysis model. Pattern-mixture models assume different patterns of missingness and analyze each pattern separately, while mixed-effects models account for both within-subject correlations and missing data.\n",
    "\n",
    "The potential consequences of using different methods to handle missing data include:\n",
    "\n",
    "Bias: Choosing an inappropriate method can introduce bias into your results, leading to incorrect conclusions about the relationships between variables.\n",
    "Loss of Statistical Power: Removing participants with missing data reduces the sample size and can result in reduced statistical power, making it harder to detect true effects.\n",
    "Type I Errors: Incorrect handling of missing data can inflate the likelihood of Type I errors (false positives) or Type II errors (false negatives).\n",
    "Invalid Inferences: Using inadequate methods can compromise the validity of your inferences and undermine the reliability of your findings.\n",
    "Misinterpretation of Results: Different methods can lead to different results, making it difficult to compare studies or draw consistent conclusions.\n",
    "Choosing an appropriate method to handle missing data depends on the nature of your data, the underlying missing data mechanism, and the assumptions you're willing to make. It's generally recommended to consult with statisticians or researchers experienced in missing data techniques to determine the most suitable approach for your specific situation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide\n",
    "an example of a situation where a post-hoc test might be necessary.\n",
    "\n",
    "\n",
    "\n",
    "Post-hoc tests are used after an analysis of variance (ANOVA) to determine which specific group means are significantly different from each other when a significant overall effect is found. ANOVA itself only tells you that there is a difference somewhere among the groups, but it doesn't identify which specific group pairs are responsible for this difference. Post-hoc tests help to pinpoint those differences.\n",
    "\n",
    "Here are some common post-hoc tests and when to use them:\n",
    "\n",
    "Tukey's Honestly Significant Difference (HSD):\n",
    "\n",
    "Use when you have conducted a one-way ANOVA.\n",
    "Suitable for situations where you have a relatively small number of groups (3 or more).\n",
    "Controls the family-wise error rate, providing a balance between controlling the Type I error and maintaining the power of the test.\n",
    "Bonferroni Correction:\n",
    "\n",
    "Use when conducting multiple pairwise comparisons.\n",
    "Suitable for situations where you have more than a few groups and need to control the family-wise error rate.\n",
    "Divides the significance level (alpha) by the number of comparisons to maintain an overall alpha level.\n",
    "Sidak Correction:\n",
    "\n",
    "Similar to the Bonferroni correction, but often used in cases where the number of comparisons is small.\n",
    "It can be more powerful than Bonferroni when the number of comparisons is low.\n",
    "Dunn's Test (also known as Dunn's Multiple Comparison Test or Dunn-Bonferroni Test):\n",
    "\n",
    "Use when you have conducted a Kruskal-Wallis test (non-parametric equivalent of ANOVA).\n",
    "Appropriate for situations with unequal group sizes or non-normal data.\n",
    "It uses a rank-based approach for comparisons.\n",
    "Holm-Bonferroni Method:\n",
    "\n",
    "Use when conducting multiple comparisons.\n",
    "Provides a step-wise adjustment of p-values to control the family-wise error rate.\n",
    "Can be more powerful than the standard Bonferroni correction.\n",
    "Example situation where a post-hoc test might be necessary:\n",
    "\n",
    "Imagine you are a researcher studying the effects of different teaching methods on student performance. You have three teaching methods (A, B, and C), and you've conducted a one-way ANOVA to determine if there is a significant difference in performance among these methods. The ANOVA results indicate a significant difference (p < 0.05).\n",
    "\n",
    "In this case, you would use a post-hoc test to determine which specific pairs of teaching methods are significantly different from each other. Let's say you choose Tukey's HSD as your post-hoc test. The test would then provide you with confidence intervals and p-values for all possible pairs of teaching methods, allowing you to identify where the significant differences lie. This information would help you make more specific and nuanced conclusions about the effects of different teaching methods on student performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from\n",
    "50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python\n",
    "to determine if there are any significant differences between the mean weight loss of the three diets.\n",
    "Report the F-statistic and p-value, and interpret the results.\n",
    "\n",
    "\n",
    "To conduct a one-way ANOVA in Python to compare the mean weight loss of three diets (A, B, and C), you can use the scipy.stats module. First, you'll need to have the weight loss data for each diet group. Let's assume you have the data in three separate arrays: weight_loss_A, weight_loss_B, and weight_loss_C.\n",
    "\n",
    "Here's how you can perform the one-way ANOVA and interpret the results:\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Weight loss data for each diet group\n",
    "weight_loss_A = [/* weight loss data for diet A */]\n",
    "weight_loss_B = [/* weight loss data for diet B */]\n",
    "weight_loss_C = [/* weight loss data for diet C */]\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(weight_loss_A, weight_loss_B, weight_loss_C)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05  # significance level\n",
    "\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"There is a significant difference between the mean weight loss of the three diets.\")\n",
    "else:\n",
    "    print(\"There is no significant difference between the mean weight loss of the three diets.\")\n",
    "\n",
    "  n this code snippet, the f_oneway function from scipy.stats is used to perform the one-way ANOVA. The F-statistic and p-value are then printed, and based on the p-value and chosen significance level (alpha), you can interpret whether there are significant differences in the mean weight loss of the three diets.\n",
    "\n",
    "If the p-value is less than the chosen significance level (e.g., 0.05), you would reject the null hypothesis and conclude that there is a significant difference in the mean weight loss of the three diets. If the p-value is greater than or equal to the significance level, you would fail to reject the null hypothesis and conclude that there is no significant difference in the mean weight loss of the three diets.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q10. A company wants to know if there are any significant differences in the average time it takes to\n",
    "complete a task using three different software programs: Program A, Program B, and Program C. They\n",
    "randomly assign 30 employees to one of the programs and record the time it takes each employee to\n",
    "complete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects or\n",
    "interaction effects between the software programs and employee experience level (novice vs.\n",
    "experienced). Report the F-statistics and p-values, and interpret the results.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "n = 30\n",
    "programs = ['A', 'B', 'C']\n",
    "experience = ['novice', 'experienced']\n",
    "data = {\n",
    "    'program': np.random.choice(programs, n),\n",
    "    'experience': np.random.choice(experience, n),\n",
    "    'time': np.random.normal(10, 2, n)  # Simulated task completion time\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "1.Perform the two-way ANOVA:\n",
    "    \n",
    "# Perform two-way ANOVA\n",
    "model = ols('time ~ C(program) + C(experience) + C(program):C(experience)', data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(model)\n",
    "print(anova_table)\n",
    "\n",
    "1.Interpret the results:\n",
    "The output of the ANOVA table will provide F-statistics and p-values for each main effect (software program, experience level) and the interaction effect between them.\n",
    "\n",
    "Main effect of software program (Program A, B, C):\n",
    "\n",
    "Null hypothesis (H0): There is no significant difference in average completion time between the software programs.\n",
    "Alternative hypothesis (H1): There is a significant difference in average completion time between at least two of the software programs.\n",
    "Main effect of experience level (novice vs. experienced):\n",
    "\n",
    "Null hypothesis (H0): There is no significant difference in average completion time between novice and experienced employees.\n",
    "Alternative hypothesis (H1): There is a significant difference in average completion time between novice and experienced employees.\n",
    "Interaction effect between software program and experience level:\n",
    "\n",
    "Null hypothesis (H0): The effect of software program on completion time does not depend on experience level.\n",
    "Alternative hypothesis (H1): The effect of software program on completion time depends on experience level.\n",
    "Interpretation:\n",
    "\n",
    "Look at the p-values for each effect and interaction in the ANOVA table.\n",
    "If the p-value is less than your chosen significance level (e.g., 0.05), you reject the null hypothesis and conclude that there's a significant effect.\n",
    "If the p-value is greater than the significance level, you fail to reject the null hypothesis, indicating no significant effect.\n",
    "\n",
    "\n",
    "\n",
    "Q11. An educational researcher is interested in whether a new teaching method improves student test\n",
    "scores. They randomly assign 100 students to either the control group (traditional teaching method) or the\n",
    "experimental group (new teaching method) and administer a test at the end of the semester. Conduct a\n",
    "two-sample t-test using Python to determine if there are any significant differences in test scores\n",
    "between the two groups. If the results are significant, follow up with a post-hoc test to determine which\n",
    "group(s) differ significantly from each other.\n",
    "\n",
    "\n",
    "1.Import the required libraries and generate some example data for demonstration purposes:\n",
    "    \n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Generating example data for control and experimental groups\n",
    "np.random.seed(42)  # For reproducibility\n",
    "control_group = np.random.normal(loc=75, scale=10, size=100)  # Example control group scores\n",
    "experimental_group = np.random.normal(loc=80, scale=10, size=100)  # Example experimental group scores\n",
    "\n",
    "1.Perform a two-sample t-test:\n",
    "\n",
    "    # Perform two-sample t-test\n",
    "t_statistic, p_value = ttest_ind(control_group, experimental_group)\n",
    "\n",
    "alpha = 0.05  # Significance level\n",
    "\n",
    "# Check if the p-value is less than the significance level\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the groups.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: No significant difference between the groups.\")\n",
    "\n",
    "    1.If the results are significant, you can proceed with a post-hoc test. One commonly used post-hoc test is the Tukey-Kramer test for comparing multiple groups. For this, you can use the statsmodels library:\n",
    "        \n",
    "        import pandas as pd\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Combine the data for the post-hoc test\n",
    "all_scores = np.concatenate((control_group, experimental_group))\n",
    "group_labels = ['Control'] * len(control_group) + ['Experimental'] * len(experimental_group)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({'Group': group_labels, 'Scores': all_scores})\n",
    "\n",
    "# Perform Tukey-Kramer post-hoc test\n",
    "tukey_results = pairwise_tukeyhsd(data['Scores'], data['Group'], alpha=0.05)\n",
    "\n",
    "print(tukey_results)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q12. A researcher wants to know if there are any significant differences in the average daily sales of three\n",
    "retail stores: Store A, Store B, and Store C. They randomly select 30 days and record the sales for each store\n",
    "on those days. Conduct a repeated measures ANOVA using Python to determine if there are any\n",
    "\n",
    "significant differences in sales between the three stores. If the results are significant, follow up with a post-\n",
    "hoc test to determine which store(s) differ significantly from each other.\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "from scipy.stats import f_oneway\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Create a DataFrame with simulated sales data\n",
    "np.random.seed(42)\n",
    "days = 30\n",
    "stores = ['Store A', 'Store B', 'Store C']\n",
    "data = {\n",
    "    'Store': np.repeat(stores, days),\n",
    "    'Sales': np.random.randint(100, 1000, size=days * len(stores))\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform repeated measures ANOVA\n",
    "rm_anova = AnovaRM(data=df, depvar='Sales', subject='Store', within=['Store'])\n",
    "rm_results = rm_anova.fit()\n",
    "\n",
    "print(\"Repeated Measures ANOVA Results:\")\n",
    "print(rm_results)\n",
    "\n",
    "# Perform one-way ANOVA (alternative approach)\n",
    "store_a = df[df['Store'] == 'Store A']['Sales']\n",
    "store_b = df[df['Store'] == 'Store B']['Sales']\n",
    "store_c = df[df['Store'] == 'Store C']['Sales']\n",
    "f_statistic, p_value = f_oneway(store_a, store_b, store_c)\n",
    "\n",
    "print(\"\\nOne-Way ANOVA Results:\")\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Perform post-hoc Tukey's HSD test\n",
    "posthoc = pairwise_tukeyhsd(df['Sales'], df['Store'], alpha=0.05)\n",
    "print(\"\\nTukey's HSD Post-Hoc Test Results:\")\n",
    "print(posthoc)\n",
    "\n",
    "\n",
    "In this code:\n",
    "\n",
    "1.We simulate sales data for three stores (Store A, Store B, Store C) over 30 days each.\n",
    "2.We perform both repeated measures ANOVA and one-way ANOVA using different approaches.\n",
    "3.We calculate the F-statistic and p-value for the ANOVA results.\n",
    "4.We perform Tukey's HSD post-hoc test to determine significant differences between the stores.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
